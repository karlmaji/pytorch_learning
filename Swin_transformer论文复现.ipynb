{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Swin transformer论文复现.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6iHN2srjgkZv2M+2a2E3t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karlmaji/pytorch_learning/blob/master/Swin_transformer%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9iYAO_A3WIC",
        "outputId": "ce0c85d2-4e28-4dfa-e3c8-b2b8821a2189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "tS5_ye7l4C-q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step1 img to patch\n",
        "- 将img中相邻的patchsize*patchsize个像素点捆绑为一个patch，输出序列的shape为[bs,num_patch,patch_depth]  \n",
        "其中patch_depth = patch_size * patch_size * input_channel\n",
        "\n"
      ],
      "metadata": {
        "id": "HHHnn1jE37cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# patch_depth = patchsize *patchsize *input_channel\n",
        "# patch_depth= 12\n",
        "# model_dim = 8\n",
        "# weight = torch.randn(12,8)\n",
        "\n",
        "\n",
        "# 方法1：使用unfold 方法\n",
        "def img2patch(img,patchsize,weight):\n",
        "  patch = F.unfold(img,kernel_size=patchsize,\n",
        "                   stride=patchsize).transpose(-1,-2) \n",
        "                   #[bs,patch_num,patch_depth]\n",
        "\n",
        "  patch_embedding = patch @ weight #weight.shape:[patch_depth,model_dim]\n",
        "\n",
        "  # print(patch_embedding.shape) #[bs,num_patch,model_dim]\n",
        "  \n",
        "  return patch_embedding\n",
        "\n",
        "# 方法2:使用卷积方法\n",
        "def img2patch_conv(img,patchsize,model_dim):\n",
        "  bs,i_c,h,w =img.shape\n",
        "\n",
        "  #实际建立model时 先定义卷积层在调用\n",
        "  layer= nn.Conv2d(3,model_dim,kernel_size=patchsize,stride=patchsize)\n",
        "  #output:[bs,model_dim,h,w]\n",
        "\n",
        "  patch_embedding=layer(img).reshape(bs,model_dim,-1).transpose(-1,-2)\n",
        "  # print(patch_embedding.shape)\n",
        "\n",
        "\n",
        "# img=torch.randn(2,3,224,224)\n",
        "# img2patch(img,2,weight)\n",
        "# img2patch_conv(img,2,model_dim)\n"
      ],
      "metadata": {
        "id": "r-dYgMqS4Bx2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step2 构建MultiHeadSelfAttention"
      ],
      "metadata": {
        "id": "0pqza3_e_Il_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self,model_dim,num_head):\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "    self.headnum = num_head\n",
        "    self.proj_Linear = nn.Linear(model_dim,3*model_dim)\n",
        "    self.final_Linear = nn.Linear(model_dim,model_dim)\n",
        "\n",
        "  def forward(self,input_,with_mask):\n",
        "    bs,seq_len,model_dim = input_.shape\n",
        "\n",
        "    num_head = self.headnum\n",
        "    head_dim = model_dim // num_head\n",
        "\n",
        "    proj_output = self.proj_Linear(input_) # [bs, seqlen, 3*model_dim] \n",
        "\n",
        "    proj_output=proj_output.reshape(bs,seq_len,3,\\\n",
        "                    num_head,head_dim).permute(2,0,3,1,4) \n",
        "                  #output:[3,bs,num_head,seq_len,head_dim]\n",
        "\n",
        "    q, k, v=proj_output.reshape(3,bs*num_head,seq_len,head_dim)[:]\n",
        "    #q,k,v .shape= [bs*num_head,seq_len,head_dim]\n",
        "    if with_mask==None:\n",
        "      atten_prob=F.softmax(torch.bmm(q,k.transpose(-1,-2)/torch.sqrt(torch.tensor(head_dim))),dim=-1)\n",
        "    else:\n",
        "      mask=torch.tile(with_mask,(num_head,1,1))\n",
        "      atten_prob=F.softmax(torch.bmm(q,k.transpose(-1,-2)/torch.sqrt(torch.tensor(head_dim))) + mask,dim=-1)\n",
        "      pass\n",
        "\n",
        "    output = torch.bmm(atten_prob, v) # [bs*num_head, seqlen, head_dim]\n",
        "    output = output.reshape(bs, num_head, seq_len, head_dim).transpose(1, 2) #[bs, seqlen, num_head, head_dim]\n",
        "    output = output.reshape(bs, seq_len, model_dim)\n",
        "\n",
        "    output = self.final_Linear(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "QH15swLq8_UY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step3 构建W-MHSA"
      ],
      "metadata": {
        "id": "RtbZ_bAMnLYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Window_MultHeadSelfAttention(patch_embedding,mhsa,window_size=4,num_head=2):\n",
        "\n",
        "  num_patch_in_window = window_size * window_size\n",
        "\n",
        "  bs, num_patch, patch_depth = patch_embedding.shape\n",
        "\n",
        "  img_height = img_width = int(torch.sqrt(torch.tensor(num_patch)))\n",
        "\n",
        "  patch_embedding = patch_embedding.transpose(-1,-2) #[bs,patch_depth,num_patch]\n",
        "\n",
        "  patch_img = patch_embedding.reshape(bs,patch_depth,img_height,img_width)\n",
        "\n",
        "  patch_windows = F.unfold(patch_img,kernel_size=window_size,stride=window_size).transpose(-1,-2) #[bs,num_windows,window_depth]\n",
        "  # window_depth = window_size * window_size * patch_depth =num_patch_in_window*patch_depth\n",
        "\n",
        "  bs,num_windows,window_depth = patch_windows.shape\n",
        "\n",
        "  patch_window = patch_windows.reshape(bs*num_windows,patch_depth,num_patch_in_window).transpose(-1,-2)\n",
        "  #[bs*num_windows,num_patch_in_window,patch_depth]\n",
        "  output = mhsa(patch_window,with_mask=None)\n",
        "\n",
        "\n",
        "  #这里输出为window的4维格式是为了方便后续的SW-MHSA\n",
        "  output=output.reshape(bs,num_windows,num_patch_in_window,patch_depth)\n",
        "\n",
        "  return output\n"
      ],
      "metadata": {
        "id": "J5aINY7xReHn"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step4 构建SW-MHSA\n"
      ],
      "metadata": {
        "id": "lGsp5X9Bx3Sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def window2img(patch_window):\n",
        "  bs,num_windows,num_patch_in_window,patch_depth = patch_window.shape\n",
        "  window_size = int(torch.sqrt(torch.tensor(num_patch_in_window)))\n",
        "  img_height = img_width = int(torch.sqrt(torch.tensor(num_windows))) * window_size\n",
        "\n",
        "  patch_img = patch_window.reshape(bs,img_height // window_size,img_width //window_size \\\n",
        "                  ,window_size,window_size,patch_depth).transpose(2,3)\n",
        "  \n",
        "  patch_img = patch_img.reshape(bs,img_height,img_width,patch_depth).permute(0,3,1,2)\n",
        "  #output [bs,patch_depth,img_height,img_width]\n",
        "  return patch_img\n",
        "\n",
        "def get_shift_window_mask(bs,window_size,img_height,img_width):\n",
        "  #we need out shape is [bs,num_windows,num_patch_in_window,num_patch_in_window]\n",
        "  index_matrix = torch.zeros(img_height, img_width)\n",
        "\n",
        "  for i in range(img_height):\n",
        "      for j in range(img_width):\n",
        "          row_times = (i+window_size//2) // window_size\n",
        "          col_times = (j+window_size//2) // window_size\n",
        "          index_matrix[i, j] = row_times*(img_width//window_size) + col_times + 1\n",
        "  index_matrix = torch.roll(index_matrix,shifts=(-window_size//2,-window_size//2),dims=(0,1))\n",
        "  index_matrix = index_matrix.unsqueeze(0).unsqueeze(0) #[bs,c,h,w]\n",
        "  mask = F.unfold(index_matrix,kernel_size=(window_size,window_size),\\\n",
        "                  stride=(window_size,window_size)).transpose(-1,-2)\n",
        "  #[bs,num_window,num_patch_in_window]\n",
        "  mask=torch.tile(mask,dims=(bs,1,1))\n",
        "\n",
        "  bs, num_window, num_patch_in_window = mask.shape\n",
        "\n",
        "  mask=mask.unsqueeze(-1) #[bs,num_window,num_patch_in_window,1]\n",
        "  mask = (mask - mask.transpose(-1,-2)) == 0 #[bs,num_window,num_patch_in_window,num_patch_in_window]\n",
        "\n",
        "  mask = mask.to(torch.float32)\n",
        "# MHSA中使用的是加法，因此True值应该为0  False 应该为负无穷\n",
        "  mask = (1-mask)*-10000\n",
        "\n",
        "  mask = mask.reshape(bs*num_window,num_patch_in_window,num_patch_in_window)\n",
        "\n",
        "  return mask\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Shift_Window_func(patch_window,shift_size,generate_mask=False):\n",
        "\n",
        "  bs,num_windows,num_patch_in_window,patch_depth = patch_window.shape\n",
        "\n",
        "  window_size = int(torch.sqrt(torch.tensor(num_patch_in_window)))\n",
        "\n",
        "  patch_img = window2img(patch_window)\n",
        "\n",
        "  bs, patch_depth, img_height, img_width = patch_img.shape\n",
        "\n",
        "  shift_img = torch.roll(patch_img,shifts=(-shift_size, -shift_size),dims=(-1,-2))\n",
        "\n",
        "#-------------------------img2window------------------------------------------#\n",
        "\n",
        "  shift_window = shift_img.permute(0,2,3,1) #[bs,img_height,img_width,patch_depth]\n",
        "  shift_window = shift_window.reshape(bs,\\\n",
        "                    img_height //window_size, \\\n",
        "                    window_size, \\\n",
        "                    img_width //window_size, \\\n",
        "                    window_size, \\\n",
        "                    patch_depth).transpose(2,3)\n",
        "\n",
        "  shift_window = shift_window.reshape(bs,num_windows,num_patch_in_window,patch_depth)\n",
        "\n",
        "#-----------------------------------------------------------------------------#\n",
        "\n",
        "  if generate_mask:\n",
        "    mask = get_shift_window_mask(bs,window_size,img_height,img_width)\n",
        "  else:\n",
        "    mask=None\n",
        "\n",
        "  return shift_window,mask\n",
        " "
      ],
      "metadata": {
        "id": "BfUkIYTZ34nJ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Shift_Window_MultHeadSelfAttention(patch_window,mhsa,num_head=2,window_size=4):\n",
        "\n",
        "  shifted_window , mask = Shift_Window_func(patch_window,shift_size = window_size//2,generate_mask=True)\n",
        "\n",
        "  bs,num_windows,num_patch_in_window,patch_depth = shifted_window.shape\n",
        "\n",
        "  shifted_window = shifted_window.reshape(bs*num_windows,num_patch_in_window,patch_depth)\n",
        "\n",
        "  mhsa_out = mhsa(shifted_window,with_mask = mask)\n",
        "\n",
        "  mhsa_out = mhsa_out.reshape(bs,num_windows,num_patch_in_window,patch_depth)\n",
        "\n",
        "  mhsa_out,_ = Shift_Window_func(mhsa_out,shift_size= - window_size // 2,generate_mask=False)\n",
        "\n",
        "  return mhsa_out\n"
      ],
      "metadata": {
        "id": "pWTbJuoptrhk"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step5 Patch Merging"
      ],
      "metadata": {
        "id": "7G9XO9ZlJmoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging(nn.Module):\n",
        "    \n",
        "    def __init__(self, model_dim, merge_size, output_depth_scale=0.5):\n",
        "        super(PatchMerging, self).__init__()\n",
        "        self.merge_size = merge_size\n",
        "        self.proj_layer = nn.Linear(\n",
        "            model_dim*merge_size*merge_size,\n",
        "            int(model_dim*merge_size*merge_size*output_depth_scale)\n",
        "        )\n",
        "        \n",
        "    def forward(self, input):\n",
        "        bs, num_window, num_patch_in_window, patch_depth = input.shape\n",
        "        window_size = int(torch.sqrt(torch.tensor(num_patch_in_window)))\n",
        "\n",
        "        input = window2img(input) #[bs, patch_depth, image_h, image_w]\n",
        "\n",
        "        merged_window = F.unfold(input, kernel_size=(self.merge_size, self.merge_size),\n",
        "                                 stride=(self.merge_size, self.merge_size)\n",
        "                                ).transpose(-1, -2)\n",
        "        merged_window = self.proj_layer(merged_window) #[bs, num_patch, new_patch_depth]\n",
        "\n",
        "        return merged_window\n",
        "    "
      ],
      "metadata": {
        "id": "jIU5pvALJmFa"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step6 构建swin transformer block"
      ],
      "metadata": {
        "id": "A6i1GrDmK3aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformerBlock(nn.Module):\n",
        "  def __init__(self,model_dim,window_size,num_head):\n",
        "    super(SwinTransformerBlock,self).__init__()\n",
        "    self.LN1 = nn.LayerNorm(model_dim)\n",
        "    self.LN2 = nn.LayerNorm(model_dim)\n",
        "    self.LN3 = nn.LayerNorm(model_dim)\n",
        "    self.LN4 = nn.LayerNorm(model_dim)\n",
        "\n",
        "    self.wsma_mlp1 = nn.Linear(model_dim, 4*model_dim)\n",
        "    self.wsma_mlp2 = nn.Linear(4*model_dim, model_dim)\n",
        "\n",
        "    self.swsma_mlp1 = nn.Linear(model_dim, 4*model_dim)\n",
        "    self.swsma_mlp2 = nn.Linear(4*model_dim, model_dim)\n",
        "\n",
        "    self.wmhsa = MultiHeadSelfAttention(model_dim,num_head)\n",
        "\n",
        "    self.swmhsa = MultiHeadSelfAttention(model_dim,num_head)\n",
        "\n",
        "  def forward(self,patch_embedding):\n",
        "    bs,num_patch,patch_depth = patch_embedding.shape\n",
        "\n",
        "    input_ = patch_embedding\n",
        "\n",
        "    patch_embedding = self.LN1(patch_embedding)\n",
        "\n",
        "    patch_window = Window_MultHeadSelfAttention(patch_embedding,self.wmhsa,window_size=4,num_head=2)\n",
        "\n",
        "    bs,num_window,num_patch_in_window,patch_depth = patch_window.shape\n",
        "\n",
        "    patch_window_output = patch_window.reshape(bs,num_patch,patch_depth)\n",
        "\n",
        "    out1 = input_ + patch_embedding\n",
        "\n",
        "    out2 = self.LN2(out1)\n",
        "    out2 = self.wsma_mlp1(out2)\n",
        "    out2 = self.wsma_mlp2(out2)\n",
        "    out2 += out1\n",
        "\n",
        "    out3 = self.LN3(out2)\n",
        "    out3_window = out3.reshape(bs,num_window,num_patch_in_window,patch_depth)\n",
        "    out3 = Shift_Window_MultHeadSelfAttention(out3_window,self.swmhsa,num_head=2,window_size=4)\n",
        "    out3 = out2 + out3.reshape(bs,num_patch,patch_depth)\n",
        "\n",
        "    out4 = self.LN4(out3)\n",
        "    out4 = self.swsma_mlp1(out4)\n",
        "    out4 = self.swsma_mlp2(out4)\n",
        "    out4 += out3\n",
        "\n",
        "    output = out4.reshape(bs,num_window,num_patch_in_window,patch_depth)\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "pDh-PdIQK28e"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "st = SwinTransformerBlock(16,4,2)\n",
        "\n",
        "x = torch.randn(3,16,16)\n",
        "\n",
        "st(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkl6v2BdQCh8",
        "outputId": "14a84002-e534-49bf-99fc-42b9737fb7f3"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 1, 16, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 构建Swin transformer model"
      ],
      "metadata": {
        "id": "KbATndvGRbxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformerModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_image_channel=3, patch_size=4, model_dim_C=8, num_classes=10,\n",
        "                 window_size=4, num_head=2, merge_size=2):\n",
        "        \n",
        "        super(SwinTransformerModel, self).__init__()\n",
        "\n",
        "        patch_depth = patch_size*patch_size*input_image_channel\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.model_dim_C = model_dim_C\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        self.patch_embedding_weight = nn.Parameter(torch.randn(patch_depth, model_dim_C)) \n",
        "        \n",
        "        self.block1 = SwinTransformerBlock(model_dim_C, window_size, num_head)\n",
        "        self.block2 = SwinTransformerBlock(model_dim_C*2, window_size, num_head)\n",
        "        self.block3 = SwinTransformerBlock(model_dim_C*4, window_size, num_head)\n",
        "        self.block4 = SwinTransformerBlock(model_dim_C*8, window_size, num_head)\n",
        "\n",
        "        \n",
        "        self.patch_merging1 = PatchMerging(model_dim_C, merge_size)\n",
        "        self.patch_merging2 = PatchMerging(model_dim_C*2, merge_size)\n",
        "        self.patch_merging3 = PatchMerging(model_dim_C*4, merge_size)\n",
        "        \n",
        "        self.final_layer = nn.Linear(model_dim_C*8, num_classes)\n",
        "    def forward(self,input_):\n",
        "\n",
        "      patch_embedding = img2patch(input_,self.patch_size,self.patch_embedding_weight)\n",
        "\n",
        "      output1 = self.block1(patch_embedding)\n",
        "\n",
        "\n",
        "      output2 = self.patch_merging1(output1)\n",
        "      output2 = self.block2(output2)\n",
        "\n",
        " \n",
        "      output3 = self.patch_merging2(output2)\n",
        "      output3 = self.block3(output3)\n",
        "\n",
        "      output4 = self.patch_merging3(output3)\n",
        "      output4 = self.block4(output4)\n",
        "\n",
        "      bs,num_window,num_patch_in_window,patch_depth = output4.shape\n",
        "      output4 = output4.reshape(bs,-1,patch_depth)\n",
        "\n",
        "      pool_output = torch.mean(output4, dim=1) #[bs, patch_depth]\n",
        "      output = self.final_layer(pool_output)\n",
        "\n",
        "      return output"
      ],
      "metadata": {
        "id": "NtTkXWsTRfui"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 难点5 分类模块，输出类别为num_classes的logits\n",
        "if __name__ == \"__main__\":\n",
        "    bs, ic, image_h, image_w = 4, 3, 256, 256\n",
        "    patch_size = 4\n",
        "    model_dim_C = 8 #一开始的patch embedding的大小\n",
        "    # max_num_token = 16\n",
        "    num_classes = 10\n",
        "    window_size = 4\n",
        "    num_head = 2\n",
        "    merge_size = 2\n",
        "    \n",
        "    patch_depth = patch_size*patch_size*ic\n",
        "    \n",
        "    image = torch.randn(bs, ic, image_h, image_w)\n",
        "    \n",
        "    model = SwinTransformerModel(ic, patch_size, model_dim_C, num_classes,\\\n",
        "                                 window_size, num_head, merge_size)\n",
        "    logits = model(image)\n",
        "    print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qKHqkMDRa-e",
        "outputId": "8ec354f1-ad10-46c4-8330-cf00340d3993"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2577e-01,  1.7471e-01,  1.1693e-01, -3.8581e-01, -1.2382e-01,\n",
            "         -1.0274e-01,  4.9425e-01,  4.9765e-01, -7.3770e-01,  4.9017e-01],\n",
            "        [-1.4078e-01,  1.4214e-01, -3.2334e-02,  3.1662e-01, -4.6753e-01,\n",
            "         -9.2532e-02,  1.0285e-01,  2.7346e-01, -1.0611e-01,  1.4083e-01],\n",
            "        [-6.6732e-01,  5.5884e-01,  1.1990e-01, -2.3074e-05, -3.3403e-01,\n",
            "         -3.6162e-01,  3.9909e-01,  1.8915e-01, -4.3181e-01,  4.4186e-01],\n",
            "        [-3.2469e-03, -1.4577e-01, -1.4872e-01, -6.9833e-02, -8.1082e-01,\n",
            "          1.6133e-01,  8.5165e-01,  3.5073e-01, -2.9628e-01, -1.2382e-01]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hyKbmx_ZU5RB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}