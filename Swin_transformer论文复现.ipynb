{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Swin transformer论文复现.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzDzudT8hErlemGC/lwN25",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karlmaji/pytorch_learning/blob/master/Swin_transformer%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9iYAO_A3WIC",
        "outputId": "67cefee1-605c-4a49-bf54-5e41789f42c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "tS5_ye7l4C-q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step1 img to patch"
      ],
      "metadata": {
        "id": "HHHnn1jE37cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# patch_depth = patchsize *patchsize *input_channel\n",
        "# patch_depth= 12\n",
        "# model_dim = 8\n",
        "# weight = torch.randn(12,8)\n",
        "\n",
        "\n",
        "# 方法1：使用unfold 方法\n",
        "def img2patch(img,patchsize,weight):\n",
        "  patch = F.unfold(img,kernel_size=patchsize,\n",
        "                   stride=patchsize).transpose(-1,-2) \n",
        "                   #[bs,patch_num,patch_depth]\n",
        "\n",
        "  patch_embedding = patch @ weight #weight.shape:[patch_depth,model_dim]\n",
        "\n",
        "  # print(patch_embedding.shape) #[bs,num_patch,model_dim]\n",
        "  \n",
        "  return patch_embedding\n",
        "\n",
        "# 方法2:使用卷积方法\n",
        "def img2patch_conv(img,patchsize,model_dim):\n",
        "  bs,i_c,h,w =img.shape\n",
        "\n",
        "  #实际建立model时 先定义卷积层在调用\n",
        "  layer= nn.Conv2d(3,model_dim,kernel_size=patchsize,stride=patchsize)\n",
        "  #output:[bs,model_dim,h,w]\n",
        "\n",
        "  patch_embedding=layer(img).reshape(bs,model_dim,-1).transpose(-1,-2)\n",
        "  # print(patch_embedding.shape)\n",
        "\n",
        "\n",
        "# img=torch.randn(2,3,224,224)\n",
        "# img2patch(img,2,weight)\n",
        "# img2patch_conv(img,2,model_dim)\n"
      ],
      "metadata": {
        "id": "r-dYgMqS4Bx2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step2 构建MultiHeadSelfAttention"
      ],
      "metadata": {
        "id": "0pqza3_e_Il_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self,model_dim,num_head):\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "    self.headnum = num_head\n",
        "    self.proj_Linear = nn.Linear(model_dim,3*model_dim)\n",
        "    self.final_Linear = nn.Linear(model_dim,model_dim)\n",
        "\n",
        "  def forward(self,input_,with_mask):\n",
        "    bs,seq_len,model_dim = input_.shape\n",
        "\n",
        "    num_head = self.headnum\n",
        "    head_dim = model_dim // num_head\n",
        "\n",
        "    proj_output = self.proj_Linear(input_) # [bs, seqlen, 3*model_dim] \n",
        "\n",
        "    proj_output = proj_output.reshape(bs,seq_len,3,\\\n",
        "                    num_head,head_dim).permute(2,0,3,1,4) \n",
        "                  #output:[3,bs,num_head,seq_len,head_dim]\n",
        "\n",
        "    q, k, v = proj_output.reshape(3,bs*num_head,seq_len,head_dim)[:]\n",
        "    #q,k,v .shape= [bs*num_head,seq_len,head_dim]\n",
        "    \n",
        "    if with_mask==None:\n",
        "      atten_prob=F.softmax(torch.bmm(q,k.transpose(-1,-2)/torch.sqrt(torch.tensor(head_dim))),dim=-1)\n",
        "\n",
        "\n",
        "    output = torch.bmm(atten_prob, v) # [bs*num_head, seqlen, head_dim]\n",
        "    output = output.reshape(bs, num_head, seq_len, head_dim).transpose(1, 2) #[bs, seqlen, num_head, head_dim]\n",
        "    output = output.reshape(bs, seq_len, model_dim)\n",
        "\n",
        "    output = self.final_Linear(output)\n",
        "    return output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QH15swLq8_UY"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BfUkIYTZ34nJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}